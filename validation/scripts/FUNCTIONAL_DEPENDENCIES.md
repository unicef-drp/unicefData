# Validation Scripts - Functional Dependencies & Data Flow

## Overview

This document shows how validation scripts depend on each other and how data flows through the system.

---

## 1. Execution Flow (When User Runs: `python run_validation.py --limit 30 --random-stratified`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execution Entry Point                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   validation/run_validation.py                                  â”‚
â”‚   â”œâ”€ Parse args: --limit 30 --random-stratified --seed 42      â”‚
â”‚   â”œâ”€ Build command: python orchestrator_indicator_tests.py ... â”‚
â”‚   â””â”€ Execute subprocess                                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Orchestration Layer                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   scripts/orchestration/orchestrator_indicator_tests.py         â”‚
â”‚   â”œâ”€ Validates TEST_SCRIPT exists                              â”‚
â”‚   â”œâ”€ Passes all args to test_all_indicators_comprehensive.py   â”‚
â”‚   â””â”€ Forwards returncode                                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Main Validation Logic                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   scripts/core_validation/test_all_indicators_comprehensive.py  â”‚
â”‚   â”œâ”€ Parse args: limit=30, random_stratified=True, seed=42    â”‚
â”‚   â”œâ”€ Load indicators from metadata (645 valid indicators)       â”‚
â”‚   â”‚                                                             â”‚
â”‚   â”œâ”€â–º SAMPLING DECISION                                        â”‚
â”‚   â”‚   â”œâ”€ If --random-stratified:                               â”‚
â”‚   â”‚   â”‚  â””â”€ Call valid_indicators_sampler.stratified_sample()  â”‚
â”‚   â”‚   â”‚     â”œâ”€ Groups 645 indicators into 18 dataflow prefixes â”‚
â”‚   â”‚   â”‚     â”œâ”€ Allocates: (count_in_prefix / 645) * 30         â”‚
â”‚   â”‚   â”‚     â”œâ”€ Enforces: minimum 1 per prefix                  â”‚
â”‚   â”‚   â”‚     â””â”€ Returns: ~30+ samples (actual depends on rule)  â”‚
â”‚   â”‚   â””â”€ Else: sequential selection (first 30)                 â”‚
â”‚   â”‚                                                             â”‚
â”‚   â””â”€ For each sampled indicator (e.g., 30-45 indicators):      â”‚
â”‚       â”‚                                                         â”‚
â”‚       â”œâ”€â–º TEST EXECUTION FOR EACH INDICATOR                    â”‚
â”‚       â”‚   â”‚                                                    â”‚
â”‚       â”‚   â”œâ”€ Check cache (Python)                              â”‚
â”‚       â”‚   â”‚  â”œâ”€ If hit: skip, load from validation/cache/python/
â”‚       â”‚   â”‚  â””â”€ If miss: execute test_python_indicator()       â”‚
â”‚       â”‚   â”‚             â””â”€ Save to cache                       â”‚
â”‚       â”‚   â”‚                                                    â”‚
â”‚       â”‚   â”œâ”€ Check cache (R)                                   â”‚
â”‚       â”‚   â”‚  â”œâ”€ If hit: skip, load from validation/cache/r/    â”‚
â”‚       â”‚   â”‚  â””â”€ If miss: execute test_r_indicator()            â”‚
â”‚       â”‚   â”‚             â””â”€ Save to cache                       â”‚
â”‚       â”‚   â”‚                                                    â”‚
â”‚       â”‚   â””â”€ Check cache (Stata)                               â”‚
â”‚       â”‚      â”œâ”€ If hit: skip, load from validation/cache/stata/
â”‚       â”‚      â””â”€ If miss: execute test_stata_indicator()        â”‚
â”‚       â”‚                  â””â”€ Save to cache                      â”‚
â”‚       â”‚                                                        â”‚
â”‚       â”œâ”€â–º CROSS-LANGUAGE VALIDATION                            â”‚
â”‚       â”‚   â””â”€ validate_cross_language.py                        â”‚
â”‚       â”‚      â”œâ”€ Compare dimensions across Python/R/Stata       â”‚
â”‚       â”‚      â”œâ”€ Compare row counts                             â”‚
â”‚       â”‚      â””â”€ Flag discrepancies                             â”‚
â”‚       â”‚                                                        â”‚
â”‚       â””â”€â–º LOG RESULTS                                          â”‚
â”‚           â”œâ”€ Per-indicator logs (in results/{TIMESTAMP}/)      â”‚
â”‚           â”œâ”€ Platform-specific logs (python/, r/, stata/)      â”‚
â”‚           â””â”€ Success/failed tracking                           â”‚
â”‚                                                                 â”‚
â”‚   Final: Generate reports                                      â”‚
â”‚   â”œâ”€ SUMMARY.md (executive summary)                            â”‚
â”‚   â”œâ”€ detailed_results.csv (full table)                         â”‚
â”‚   â”œâ”€ error_log.txt (all errors)                                â”‚
â”‚   â””â”€ Per-platform results                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Sampling System (Stratified vs Sequential)

### WITHOUT --random-stratified
```
SEQUENTIAL SAMPLING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

645 Valid Indicators
    â†“
    â”œâ”€ CME_MRY0T4
    â”œâ”€ CME_DPTM_PC
    â”œâ”€ CME_PTMR1
    â”œâ”€ COD_DIAR_TREAT
    â”œâ”€ COD_MORT_NEONAT
    â””â”€ ... (first 30)

Sample Size: exactly 30
```

### WITH --random-stratified
```
STRATIFIED SAMPLING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

645 Valid Indicators
    â†“
    â””â”€â†’ valid_indicators_sampler.py :: stratified_sample()
        â”œâ”€ Group by prefix (first part before underscore):
        â”‚
        â”‚   CME: 38 indicators  â”
        â”‚   COD: 83 indicators  â”‚
        â”‚   DM:  30 indicators  â”‚
        â”‚   ECD:  8 indicators  â”‚
        â”‚   ... (14 more)       â”‚ Total: 645
        â”‚   WT:   6 indicators  â”˜
        â”‚
        â”œâ”€ Allocate proportionally with minimum 1:
        â”‚   CME: (38/645)*30 = 1 sample   (min 1)
        â”‚   COD: (83/645)*30 = 3 samples  (rounded)
        â”‚   DM:  (30/645)*30 = 1 sample   (min 1)
        â”‚   ECD:  (8/645)*30 = 1 sample   (min 1)
        â”‚   ...
        â”‚   WT:   (6/645)*30 = 1 sample   (min 1)
        â”‚
        â””â”€ Randomly select within each group (using seed=42)

Total Sample Size: ~36-45 (exceeds limit=30 due to min 1/prefix rule)
Guarantee: All 18 dataflow prefixes represented
```

---

## 3. Cache System

```
CACHE ARCHITECTURE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

validation/cache/
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ CME_MRY0T4.csv                 â† Cached result
â”‚   â”œâ”€â”€ CME_MRY0T4.metadata.json       â† Metadata (timestamp, rows)
â”‚   â”œâ”€â”€ COD_DIAR_TREAT.csv
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ r/
â”‚   â”œâ”€â”€ CME_MRY0T4.csv
â”‚   â”œâ”€â”€ CME_MRY0T4.metadata.json
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ stata/
    â”œâ”€â”€ CME_MRY0T4.csv
    â”œâ”€â”€ CME_MRY0T4.metadata.json
    â””â”€â”€ ...

CACHE MANAGER LOGIC (cache_manager.py)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For each indicator & platform:
    1. Check if file exists
    2. Read metadata.json (timestamp, row count)
    3. Check TTL (staleness):
       â”œâ”€ If < 7 days old: USE (cache hit)
       â””â”€ If > 7 days old: SKIP (cache miss, re-fetch)
    4. Return: (cache_hit: bool, data: DataFrame)

When cache_hit = False:
    â”œâ”€ Execute Python/R/Stata test
    â”œâ”€ Save result to cache/{platform}/INDICATOR.csv
    â”œâ”€ Save metadata: {timestamp, row_count, sha256}
    â””â”€ Log: "Cached new result for INDICATOR"

When cache_hit = True:
    â”œâ”€ Load from cache
    â””â”€ Log: "Used cached result for INDICATOR"
```

---

## 4. Issue Validity Checking Flow

```
ISSUE VALIDITY SYSTEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User: .\run_issue_validity_check.ps1
    â†“
Activate Python venv
    â†“
scripts/issue_validity/check_issues_validity.py
    â”‚
    â”œâ”€ Load test indicators (small set, ~6-8 indicators)
    â”‚
    â”œâ”€ For each test indicator:
    â”‚   â”‚
    â”‚   â”œâ”€ Fetch from Python
    â”‚   â”œâ”€ Fetch from R
    â”‚   â””â”€ Fetch from Stata
    â”‚
    â””â”€ Run Issue Checks:
        â”‚
        â”œâ”€ ISSUE 1: Stata Duplicate Columns
        â”‚   â””â”€ Count columns in Stata output
        â”‚       â”œâ”€ If unique: âœ… FIXED
        â”‚       â””â”€ If duplicates: ğŸ”´ STILL_VALID
        â”‚
        â”œâ”€ ISSUE 2: Missing Dimensions (Python/R)
        â”‚   â””â”€ Compare column count across platforms
        â”‚       â”œâ”€ If ratio ~1.0: âœ… FIXED
        â”‚       â””â”€ If ratio > 2.0: ğŸ”´ STILL_VALID
        â”‚
        â”œâ”€ ISSUE 3: Row Count Discrepancies
        â”‚   â””â”€ Compare row counts
        â”‚       â”œâ”€ If all match: âœ… FIXED
        â”‚       â””â”€ If some differ: ğŸ”´ STILL_VALID
        â”‚
        â””â”€ ISSUE 4: UTF-8 Encoding
            â””â”€ Check for encoding errors
                â”œâ”€ If none: âœ… NO_ENCODING_ISSUES
                â””â”€ If detected: ğŸ”´ DETECTED

Generate Report:
â”œâ”€â”€ validation/results/issue_validity/{TIMESTAMP}/
â”œâ”€â”€ â”œâ”€â”€ issue_validity_report.txt        â† Human readable
â”œâ”€â”€ â”œâ”€â”€ issue_validity_results.json      â† Machine readable
â””â”€â”€ â””â”€â”€ tmp/                              â† Debug files
```

---

## 5. Metadata Sync Flow

```
METADATA SYNCHRONIZATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

User: python scripts/metadata_sync/orchestrator_metadata.py
    â†“
orchestrator_metadata.py
    â”‚
    â”œâ”€ Python Sync
    â”‚   â””â”€ sync_metadata_python.py
    â”‚       â”œâ”€ Fetch from UNICEF SDMX API
    â”‚       â”‚   â”œâ”€ Dataflows
    â”‚       â”‚   â”œâ”€ Indicator codelist
    â”‚       â”‚   â”œâ”€ Countries
    â”‚       â”‚   â””â”€ Regions
    â”‚       â””â”€ Output: validation/scripts/metadata/current/_unicefdata_*.yaml
    â”‚
    â”œâ”€ R Sync
    â”‚   â””â”€ sync_metadata_r.R
    â”‚       â”œâ”€ R package: unicefData
    â”‚       â””â”€ Output: validation/scripts/metadata/current/r_metadata.yaml
    â”‚
    â”œâ”€ Stata Sync
    â”‚   â””â”€ sync_metadata_stata.do
    â”‚       â”œâ”€ via Stata package
    â”‚       â””â”€ Output: validation/scripts/metadata/current/stata_metadata.yaml
    â”‚
    â”œâ”€ Validation
    â”‚   â”œâ”€ check_dataflows.py
    â”‚   â”‚   â””â”€ Validate DSDs (Data Structure Definitions)
    â”‚   â”œâ”€ check_sdmx_structure.py
    â”‚   â”‚   â””â”€ Validate SDMX format
    â”‚   â””â”€ check_tier_preservation.py
    â”‚       â””â”€ Ensure Tier 1 indicators preserved
    â”‚
    â””â”€ Final Metadata State
        â””â”€ validation/scripts/metadata/current/
            â”œâ”€â”€ _unicefdata_dataflows.yaml      (18 prefixes, 645 indicators)
            â”œâ”€â”€ _unicefdata_indicators.yaml     (full indicator registry)
            â”œâ”€â”€ _unicefdata_countries.yaml      (296 countries)
            â”œâ”€â”€ _unicefdata_regions.yaml        (regions)
            â””â”€â”€ _unicefdata_codelists.yaml      (dimensions)
```

---

## 6. Class Dependencies

```
CORE VALIDATION CLASSES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

test_all_indicators_comprehensive.py
â”œâ”€â”€ class TestAllIndicatorsComprehensive
â”‚   â”œâ”€â”€ __init__(args, config)
â”‚   â”œâ”€â”€ load_indicators()
â”‚   â”‚   â””â”€ Uses: metadata_sync/_unicefdata_indicators.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ sample_indicators()
â”‚   â”‚   â”œâ”€ If args.random_stratified:
â”‚   â”‚   â”‚  â””â”€ Uses: ValidIndicatorSampler.stratified_sample()
â”‚   â”‚   â””â”€ Else: sequential selection
â”‚   â”‚
â”‚   â”œâ”€â”€ test_indicator(indicator_code)
â”‚   â”‚   â”œâ”€ Uses: cache_manager.check_cache()
â”‚   â”‚   â”œâ”€ Uses: cached_test_runners.test_python()
â”‚   â”‚   â”œâ”€ Uses: cached_test_runners.test_r()
â”‚   â”‚   â”œâ”€ Uses: cached_test_runners.test_stata()
â”‚   â”‚   â””â”€ Uses: validate_cross_language.compare()
â”‚   â”‚
â”‚   â””â”€â”€ generate_reports()
â”‚       â””â”€ Outputs: SUMMARY.md, detailed_results.csv, error_log.txt
â”‚
â”œâ”€â”€ class ValidIndicatorSampler (valid_indicators_sampler.py)
â”‚   â”œâ”€â”€ __init__(allow_unknown_prefixes, verbose, use_cache_validation)
â”‚   â””â”€â”€ stratified_sample(indicators, n, seed)
â”‚       â””â”€ Returns: {prefix: [indicator1, indicator2, ...], ...}
â”‚
â”œâ”€â”€ class CacheManager (cache_manager.py)
â”‚   â”œâ”€â”€ __init__(cache_root)
â”‚   â”œâ”€â”€ check_cache(platform, indicator)
â”‚   â”‚   â””â”€ Returns: (hit: bool, data: DataFrame)
â”‚   â””â”€â”€ save_to_cache(platform, indicator, data)
â”‚
â”œâ”€â”€ class CachedTestRunners (cached_test_runners.py)
â”‚   â”œâ”€â”€ test_python_indicator(indicator_code)
â”‚   â”œâ”€â”€ test_r_indicator(indicator_code)
â”‚   â””â”€â”€ test_stata_indicator(indicator_code)
â”‚
â””â”€â”€ class ValidateCrossLanguage (validate_cross_language.py)
    â”œâ”€â”€ compare_dimensions(python_df, r_df, stata_df)
    â”œâ”€â”€ compare_rows(python_df, r_df, stata_df)
    â””â”€â”€ flag_discrepancies()
```

---

## 7. Data Flows (What Data Moves Where)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA FLOW DURING VALIDATION RUN                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Inputs:                                                         â”‚
â”‚ â”œâ”€ Metadata: validation/scripts/metadata/current/*.yaml         â”‚
â”‚ â””â”€ Cache:    validation/cache/{python,r,stata}/                â”‚
â”‚                                                                 â”‚
â”‚ Processing:                                                     â”‚
â”‚ 1. Read metadata (645 indicators)                               â”‚
â”‚ 2. Sample indicators (stratified or sequential)                 â”‚
â”‚ 3. For each sampled indicator:                                  â”‚
â”‚    â”œâ”€ Check cache (hit? use : fetch from API)                  â”‚
â”‚    â””â”€ Execute test (Python/R/Stata)                            â”‚
â”‚ 4. Compare results across platforms                             â”‚
â”‚ 5. Generate reports                                             â”‚
â”‚                                                                 â”‚
â”‚ Outputs:                                                        â”‚
â”‚ â”œâ”€ Cache:   validation/cache/{python,r,stata}/                 â”‚
â”‚ â”‚           â””â”€ INDICATOR.csv + .metadata.json                  â”‚
â”‚ â”œâ”€ Reports: validation/results/{TIMESTAMP}/                    â”‚
â”‚ â”‚           â”œâ”€ SUMMARY.md                                      â”‚
â”‚ â”‚           â”œâ”€ detailed_results.csv                            â”‚
â”‚ â”‚           â”œâ”€ error_log.txt                                   â”‚
â”‚ â”‚           â”œâ”€ python/test_log.txt (+ success/failed/)          â”‚
â”‚ â”‚           â”œâ”€ r/test_log.txt (+ success/failed/)               â”‚
â”‚ â”‚           â””â”€ stata/test_log.txt (+ success/failed/)           â”‚
â”‚ â””â”€ Logs:    validation/logs/ (per-language, per-indicator)      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. File Dependencies (Which files depend on which)

```
DEPENDENCY GRAPH
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

run_validation.py (entry point)
    â””â”€â†’ orchestrator_indicator_tests.py
        â””â”€â†’ test_all_indicators_comprehensive.py
            â”œâ”€â†’ valid_indicators_sampler.py
            â”‚   â””â”€ Metadata: _unicefdata_indicators.yaml
            â”œâ”€â†’ cache_manager.py
            â”‚   â””â”€ Cache dirs: validation/cache/{python,r,stata}/
            â”œâ”€â†’ cached_test_runners.py
            â”‚   â”œâ”€â†’ Python API client
            â”‚   â”œâ”€â†’ R unicefData package
            â”‚   â””â”€â†’ Stata unicefdata.ado
            â””â”€â†’ validate_cross_language.py
                â””â”€ Compares Python/R/Stata outputs

Metadata Sync:
    orchestrator_metadata.py
    â”œâ”€â†’ sync_metadata_python.py (SDMX API)
    â”œâ”€â†’ sync_metadata_r.R (R package)
    â”œâ”€â†’ sync_metadata_stata.do (Stata package)
    â”œâ”€â†’ check_dataflows.py
    â”œâ”€â†’ check_sdmx_structure.py
    â””â”€â†’ check_tier_preservation.py

Issue Validity:
    run_issue_validity_check.ps1
    â””â”€â†’ check_issues_validity.py
        â””â”€â†’ cached_test_runners.py

Platform Tests:
    platform_tests/*.do
    platform_tests/*.R
    (standalone tests, no cross-dependencies)
```

---

## Summary

**Total Scripts in Production:** 28  
**Total Logical Modules:** 5 (core_validation, orchestration, metadata_sync, issue_validity, platform_tests)  
**Main Entry Point:** `validation/run_validation.py`  
**Core Logic:** `scripts/core_validation/test_all_indicators_comprehensive.py`  
**Key Innovation:** Stratified sampling by dataflow prefix + intelligent caching  

